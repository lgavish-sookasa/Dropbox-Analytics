Reflection:
The basic logic was trivial - naive DFS with small considerations for edge cases; but, when considering how to scale up the analytics, some problem cases were encountered. The codebase was unscalable because of hard-coded tuples, unreadable code, and huge abstraction violations. I refactored to make the code OO. Unfortunately, this is not enough to do real-time analysis on many users and files. My logic requires that the data it analyzes be static, which is never the case for live services. For a moderate amount of users, say, 5000, it would suffice to distribute the users among several processes. At the beginning of processing each user, a hash of the root folder should be kept, and at the end of processing all the users belonging to a process and perhaps at other points depending on load-balancing schemes, these hashes should be checked to current metadata hashes to confirm no data has been changed. Any conflicted root folder should be re-analyzed. For users with many files, say 30,000, a fringe should kept that is accessed by multiple proccesses. Everytime explore() recurses, that path is pushed onto a fringe and when a process is done analyzing a folder, that path should be popped off the fringe and analyzed by that process. In this way, multiple processes can work on the same user's data at the same time. I was ambiguous with suggesting fringe, because I think a BFS would work better for scalability. If there is a hash conflict when verifying that no data has been changed, it would be wasteful to re-analyze starting from the root, '/'. With a BFS, logic could be added to find the lowest root where the hashes differentiate, thus reducing repeated work. This would require storing each subfolder level in a sepearate data-structure within the list of folders and some other trivial logic management. Another thought I had was that if a global Queue is used for concurrency, some callback functionality should be implemented. I have been hesitant to mention run-time optimization so far because my suggested methods are near optimal, if not optimal, but their practicality could vary depending on implementation. I don't know how much memory or time could be saved if I would have used tuples and vectors instead of objects; but, for the sake of readability and scalability, I decided to stick with OO. For example adding hash verification functionality takes only a few lines of code to the Content class since the Dropbox API already implements that feature. I'm not entirely sure how to measure the performance of my code and the API access, except perhaps by using timestamps - of which there are several libraries to choose from e.g. High Resolution Time, which is W3C standard if I chose to use JavaScript for callback functionalities.
